left_join(read.csv("MA Map units by MLRA.csv")
%>% mutate(mukey = as.factor(mukey))
) %>%
mutate(mlra = ifelse(Mapunit.Type == "MLRA Map Unit", 1, 0)) %>%
select(mukey, cokey, compname, comppct_r, cover, horizon_count, o_horizon, soc100_999, mlra)
library(tidyverse)
library(here)
library(ggthemes)
library(scales)
library(patchwork)
library(readxl)
library(sf)
here()
options(scipen = 999)
# OK here we go
# Filter out water and urban
# 64.7% coverage by MLRA map units
# MLRA major components that have an NA for O horizon account for .1% of MLRA area
### As best I can tell so far:
### 1. MLRA dom comps account for about 51%
### 2. nonMLRA dom comps that share a component name with an MLRA dom comp account for another 12%
##### Variation of SOC within components that share a name looks acceptable
### 3. nonMLRA dom comps that share a taxonomic class with an MLRA dom comp account for about 23.5%
##### Variation of SOC within components that share a tax class looks acceptable.
### 4. Matching based on taxonomic subgroup does not get us any additional components, unfortunately.
# That looks like about 86%, what about the remaining 14%?
###
# Establish connection to database
library(RPostgres)
library(DBI)
#
con <- dbConnect(RPostgres::Postgres(),
dbname = 'RSF',
host = 'localhost',
port = 5432, # or any other port specified by your DBA
user = 'postgres',
password = 'password')
# https://www.r-bloggers.com/2017/05/rpostgresql-and-schemas/
# List tables associated with a specific schema
dbGetQuery(con,
"SELECT table_name FROM information_schema.tables
WHERE table_schema='SSURGO-MA'")
dbListFields(con, Id(schema = "SSURGO-MA", table = "mapunit")) # YES!
mu_raw <- dbGetQuery(con, 'SELECT * FROM "SSURGO-MA"."mapunit"') %>%
filter(!str_detect(muname, fixed("water", ignore_case=T)))
mukey_nowater <-   mu_raw %>% pull(mukey)
component_all <- dbGetQuery(con, 'SELECT * FROM "SSURGO-MA"."component"')
# MRLA designation at mapunit level
mlra_raw <- read.csv("MA Map units by MLRA.csv") %>%
mutate(mukey = as.character(mukey),
mlra = Mapunit.Type == "MLRA Map Unit") %>%
select(mukey, Mapunit.Name, mlra) %>% rename_with(~ str_to_lower(.)) %>%
filter(mukey %in% mukey_nowater)
# MRLA designation at mapunit level
mlra_raw <- read.csv("MA Map units by MLRA.csv") %>%
mutate(mukey = as.character(mukey),
mlra = Mapunit.Type == "MLRA Map Unit") %>%
select(mukey, Mapunit.Name, mlra) %>% rename_with(~ str_to_lower(.)) %>%
filter(mukey %in% mukey_nowater)
here()
library(tidyverse)
library(terra)
library(randomForest)
library(randomForestSRC)
# library(parallel)
library(patchwork)
library(beepr)
library(here)
library(sf)
library(ggthemes)
here()
#
# df <- st_read(here("Random_Points_20k each 250411.gpkg")) %>%
#   mutate(Wetland = ifelse(Wetland == "Y", 1, 0)) %>%
#   mutate(Wetland = as.factor(Wetland))
#
# # from chatGPT
# 1. Define paths
input_dir <- "/Users/RSF/Library/CloudStorage/GoogleDrive-rafterf@regenerativedesigngroup.com/Shared drives/RDG_TEAM_Drive/Projects_RDG/Planning_Wing Projects/No Net Loss Wetlands C_DEP/GIS_DEP/Statewide_Layers/Tiled_Inputs"  # <- UPDATE THIS
points_path <- "/Users/RSF/Library/CloudStorage/GoogleDrive-rafterf@regenerativedesigngroup.com/Shared drives/RDG_TEAM_Drive/Projects_RDG/Planning_Wing Projects/CEI Wetlands Mapping Project_CEIWM/CEI Wetland Mapping/Preliminary Model/Random_Points_20k each 250411.gpkg"  # <- UPDATE THIS
layer_name <- "merged"  # <- UPDATE THIS if needed
# 2. Read points
gpkg_points <- st_read(points_path, layer = layer_name)  %>%
mutate(Wetland = ifelse(Wetland == "Y", 1, 0),
wetland = as.factor(Wetland),
id = paste(Wetland, rand_point_id, sep="_")) %>%
select(id, wetland, geom)
# Convert to terra object for extract()
points_vect <- vect(gpkg_points)
# 3. List all .vrt files
vrt_files <- list.files(input_dir, pattern = "\\.vrt$", recursive = F, full.names = TRUE)
vrt_files[3]
vrt_files[6]
vrt_files[7]
raster <- rast(vrt_file)
vrt_file = vrt_files[7]
raster <- rast(vrt_file)
var_name <- tools::file_path_sans_ext(basename(vrt_file))
vals <- extract(raster, points_vect)[[2]]  # [[2]] skips ID col
# # 4. Loop over VRTs and extract values
all_values <- list()
all_values[[var_name]] <- vals
# 4. Setup parallel + progress
library(future)
plan(multisession)
library(pbapply)
pboptions(type = "timer")
raster <- rast(vrt_file)
var_name <- tools::file_path_sans_ext(basename(vrt_file))
vals <- extract(raster, points_vect)[[2]]
vals
names(vals) <- var_name
names(vals)
var_name
names(vals) <- var_name
names(vals)
extract_one <- function(vrt_file) {
raster <- rast(vrt_file)
var_name <- tools::file_path_sans_ext(basename(vrt_file))
vals <- extract(raster, points_vect)[[2]]
names(vals) <- var_name
return(vals)
}
extract_one(vrt_files[7])
tst <- extract_one(vrt_files[7])
# 5. Helper function to extract one VRT
extract_one <- function(vrt_file) {
raster <- rast(vrt_file)
var_name <- tools::file_path_sans_ext(basename(vrt_file))
vals <- extract(raster, points_vect)[[2]]
names(vals) <- var_name
return(vals)
}
# 6. Parallel extraction with progress bar
all_values_list <- pblapply(vrt_files[7], extract_one)
vrt_files[idx]
idx <- c(3, 5:10)
vrt_files[idx]
# 6. Parallel extraction with progress bar
all_values_list <- pblapply(vrt_files[idx], extract_one)
library(tidyverse)
library(terra)
library(randomForest)
library(randomForestSRC)
# library(parallel)
library(patchwork)
library(beepr)
library(here)
library(sf)
library(ggthemes)
here()
# 1. Define paths
input_dir <- "/Users/RSF/Library/CloudStorage/GoogleDrive-rafterf@regenerativedesigngroup.com/Shared drives/RDG_TEAM_Drive/Projects_RDG/Planning_Wing Projects/No Net Loss Wetlands C_DEP/GIS_DEP/Statewide_Layers/Tiled_Inputs"  # <- UPDATE THIS
points_path <- "/Users/RSF/Library/CloudStorage/GoogleDrive-rafterf@regenerativedesigngroup.com/Shared drives/RDG_TEAM_Drive/Projects_RDG/Planning_Wing Projects/CEI Wetlands Mapping Project_CEIWM/CEI Wetland Mapping/Preliminary Model/Random_Points_20k each 250411.gpkg"  # <- UPDATE THIS
layer_name <- "merged"  # <- UPDATE THIS if needed
# 2. Read points
gpkg_points <- st_read(points_path, layer = layer_name)  %>%
mutate(Wetland = ifelse(Wetland == "Y", 1, 0),
wetland = as.factor(Wetland),
id = paste(Wetland, rand_point_id, sep="_")) %>%
select(id, wetland, geom)
# Convert to terra object for extract()
points_vect <- vect(gpkg_points)
# 3. List all .vrt files
vrt_files <- list.files(input_dir, pattern = "\\.vrt$", recursive = F, full.names = TRUE)
# Trying this with parallel processing and progress bars, below....
# # 4. Loop over VRTs and extract values
# all_values <- list()
#
# for (vrt_file in vrt_files) {
#   vrt_file = vrt_files[7]
#   raster <- rast(vrt_file)
#   var_name <- tools::file_path_sans_ext(basename(vrt_file))
#   vals <- extract(raster, points_vect)[[2]]  # [[2]] skips ID col
#   all_values[[var_name]] <- vals
# }
# 5. Bind all extracted values
# values_df <- as.data.frame(all_values)
# Combine with original point IDs or geometry
# output <- bind_cols(gpkg_points, values_df)
#
# # 6. Save output
# st_write(output, "/path/to/output_with_raster_values.gpkg", delete_dsn = TRUE)
# # OR
# write.csv(output, "/path/to/output_with_raster_values.csv", row.names = FALSE)
# 4. Setup parallel + progress
library(future)
plan(multisession)
library(pbapply)
pboptions(type = "timer")
# 5. Helper function to extract one VRT
extract_one <- function(vrt_file) {
raster <- rast(vrt_file)
var_name <- tools::file_path_sans_ext(basename(vrt_file))
vals <- extract(raster, points_vect)[[2]]
names(vals) <- var_name
return(vals)
}
idx <- c(3, 5:10)
# 6. Parallel extraction with progress bar
all_values_list <- pblapply(vrt_files[idx], extract_one)
# 7. Combine all extracted values
values_df <- as.data.frame(all_values_list)
View(values_df)
View(all_values_list)
vrt_file = vrt_files[7]
raster <- rast(vrt_file)
var_name <- tools::file_path_sans_ext(basename(vrt_file))
vals <- extract(raster, points_vect)
View(vals)
vals <- extract(raster, points_vect)[[2]]
names(vals)
extract_one <- function(vrt_file) {
raster <- rast(vrt_file)
var_name <- tools::file_path_sans_ext(basename(vrt_file))
vals <- extract(raster, points_vect)[[2]]
df <- data.frame(vals)
names(df) <- var_name
return(df)
}
idx <- c(3, 5:10)
# 6. Parallel extraction with progress bar
all_values_list <- pblapply(vrt_files[idx], extract_one)
# 7. Combine all extracted values
values_df <- as.data.frame(all_values_list)
View(values_df)
values_df2 <- bind_cols(all_values_list)
values_df == values_df2
table(values_df == values_df2)
View(values_df)
View(values_df2)
library(tidyverse)
library(terra)
library(here)
library(sf)
library(purrr)
here()
################ Generate random points -----
# ---- Parameters ----
gdb_path <- "/Users/RSF/Library/CloudStorage/GoogleDrive-rafterf@regenerativedesigngroup.com/Shared drives/RDG_TEAM_Drive/Projects_RDG/Planning_Wing Projects/CEI Wetlands Mapping Project_CEIWM/CEI Wetland Mapping/Statewide_Wetlands_Digitized_by_CEI_20250403/89e363ea-273f-4d29-beb4-e749997245a1.gdb"   # Update to actual path
layer_name <- "Wetlands_Digitized_by_CEI"      # Update to actual layer name
group_var <- "Type"                   # Field in your GDB (e.g., "Wetland", "Upland")
n_per_group <- 5000                    # How many random points per group
# ---- Read polygons from GDB ----
polygons <- st_read(gdb_path, layer = layer_name)
polygons %>%
# group_by(!!sym(group_var)) %>%
group_by(group_var) %>%
group_split()
polygons %>%
group_by(!!sym(group_var)) %>%
group_split()
sampled_points <-
polygons %>%
group_by(!!sym(group_var)) %>%
group_split() %>%
map_dfr(function(group_df) {
group_name <- unique(group_df[[group_var]])
merged_geom <- st_union(group_df)
# Generate random points within the merged polygon
points <- st_sample(merged_geom, size = n_per_group, type = "random")
st_sf(
!!group_var := group_name,
geometry = points
)
})
n_per_group <- 5                    # How many random points per group
n_per_group <- 1                    # How many random points per group
sampled_points <-
polygons %>%
group_by(!!sym(group_var)) %>%
group_split() %>%
map_dfr(function(group_df) {
group_name <- unique(group_df[[group_var]])
merged_geom <- st_union(group_df)
# Generate random points within the merged polygon
points <- st_sample(merged_geom, size = n_per_group, type = "random")
st_sf(
!!group_var := group_name,
geometry = points
)
})
library(tidyverse)
library(terra)
library(here)
library(sf)
library(purrr)
here()
library(tidyverse)
library(terra)
library(here)
library(sf)
library(purrr)
here()
library(tidyverse)
library(terra)
library(randomForest)
library(randomForestSRC)
# library(parallel)
library(patchwork)
library(beepr)
library(here)
library(sf)
library(ggthemes)
here()
options(scipen = 999)
############## read in training and test data -----
df <- st_read(here("sample_points_w_predictors_5000.gpkg")) %>%
mutate(wetland = as.factor(wetland), id = NULL) %>%
st_drop_geometry()
library(tidyverse)
library(terra)
library(here)
library(sf)
library(purrr)
here()
options(scipen = 999)
input_dir <- "/Users/RSF/Library/CloudStorage/GoogleDrive-rafterf@regenerativedesigngroup.com/Shared drives/RDG_TEAM_Drive/Projects_RDG/Planning_Wing Projects/No Net Loss Wetlands C_DEP/GIS_DEP/Statewide_Layers/Tiled_Inputs"
points_path <- "sampled_points_with_attr.gpkg"
layer_name <- "merged"
sampled_points_with_class <- st_read("sampled_points_with_attr.gpkg")
points_path <- "sampled_points_with_attr.gpkg"
library(tidyverse)
library(terra)
library(here)
library(sf)
library(purrr)
here()
options(scipen = 999)
################ Generate random points, get wetland class from DEP where available -----
# ---- Parameters
gdb_path <- "/Users/RSF/Library/CloudStorage/GoogleDrive-rafterf@regenerativedesigngroup.com/Shared drives/RDG_TEAM_Drive/Projects_RDG/Planning_Wing Projects/CEI Wetlands Mapping Project_CEIWM/CEI Wetland Mapping/Statewide_Wetlands_Digitized_by_CEI_20250403/89e363ea-273f-4d29-beb4-e749997245a1.gdb"        # path to your .gdb
gdb_layer <- "Wetlands_Digitized_by_CEI"         # name of layer inside the gdb
shapefile_path <- "/Users/RSF/Library/CloudStorage/GoogleDrive-rafterf@regenerativedesigngroup.com/Shared drives/RDG_TEAM_Drive/RDG_GIS_DATA/STATE/Massachusetts/Regulated Area/wetlandsdep 240425/wetlandsdep/WETLANDSDEP_POLY.shp"  # path to .shp file
group_var <- "Type"
n_per_group <- 2500
#
# # --- Load
new_polygons <- st_read(gdb_path, layer = gdb_layer) %>% st_make_valid()
old_polygons <- st_read(shapefile_path) %>% st_make_valid()
# # --- Prep
if (st_crs(new_polygons) != st_crs(old_polygons)) {
old_polygons <- st_transform(old_polygons, st_crs(new_polygons))
}
# Join attributes (example using st_join)
joined_data <- st_join(new_polygons, old_polygons %>% select(IT_VALDESC), largest=TRUE)
# # ---- Group and generate random points
system.time( # 11 minutes for 5000 points
sampled_points <-
joined_data %>%
group_by(!!sym(group_var)) %>%
group_split() %>%
map_dfr(function(group_df) {
group_name <- unique(group_df[[group_var]])
merged_geom <- st_union(group_df)
# Generate random points within the merged polygon
points <- st_sample(merged_geom, size = n_per_group, type = "random")
st_sf(
group_type = rep(group_name, length(points)),
geometry = points
)
})
)
sampled_points_with_class <-
st_join(sampled_points, old_polygons %>% select(IT_VALDESC))
# ---- Output
st_write(sampled_points_with_class, "sampled_points_with_class.gpkg")
input_dir <- "/Users/RSF/Library/CloudStorage/GoogleDrive-rafterf@regenerativedesigngroup.com/Shared drives/RDG_TEAM_Drive/Projects_RDG/Planning_Wing Projects/No Net Loss Wetlands C_DEP/GIS_DEP/Statewide_Layers/Tiled_Inputs"
points_path <- "sampled_points_with_class.gpkg"
layer_name <- "merged"
# 2. Read points
gpkg_points <- st_read(points_path)  %>%
mutate(wetland = ifelse(group_type == "Wetland", 1, 0),
wetland = as.factor(wetland),
group_type = NULL)
################ Get predictor data for points ------
# 1. Define paths
input_dir <- "/Users/RSF/Library/CloudStorage/GoogleDrive-rafterf@regenerativedesigngroup.com/Shared drives/RDG_TEAM_Drive/Projects_RDG/Planning_Wing Projects/No Net Loss Wetlands C_DEP/GIS_DEP/Statewide_Layers/Tiled_Inputs"
points_path <- "sampled_points_with_class.gpkg"
layer_name <- "merged"
gpkg_points <- st_read(points_path)  %>%
mutate(wetland = ifelse(group_type == "Wetland", 1, 0),
wetland = as.factor(wetland),
group_type = NULL)
library(future)
plan(multisession)
library(pbapply)
pboptions(type = "timer")
# 4. List all .vrt files
vrt_files <- list.files(input_dir, pattern = "\\.vrt$", recursive = F, full.names = TRUE)
# 5. Convert points to terra object and match CRS for extract()
points_vect <- vect(gpkg_points) %>% project(y=crs(rast(vrt_files[1])))
# 6. Create extaction function to apply across list of .vrt files
extract_one <- function(vrt_file) {
raster <- rast(vrt_file)
crs(raster)
crs(points_vect)
var_name <- tools::file_path_sans_ext(basename(vrt_file))
vals <- extract(raster, points_vect)[[2]]
df <- data.frame(vals)
names(df) <- var_name
return(df)
}
# 7. Parallel extraction with progress bar
all_values_list <- pblapply(vrt_files, extract_one)
# 8. Combine all extracted values
values_df <- as.data.frame(all_values_list)
# Combine with original point IDs or geometry
output <- bind_cols(gpkg_points, values_df)
View(output)
# 9. Save output
traindata_file = paste0("sample_points_w_predictors_and_class", nrow(gpkg_points))
st_write(output, paste0(traindata_file, ".gpkg"), delete_dsn = TRUE)
write.csv(output, paste0(traindata_file, ".csv"), row.names = FALSE)
View(output)
traindata_file
# 9. Save output
traindata_file = paste0("sample_points_w_predictors_and_class_", nrow(gpkg_points))
st_write(output, paste0(traindata_file, ".gpkg"), delete_dsn = TRUE)
write.csv(output, paste0(traindata_file, ".csv"), row.names = FALSE)
here()
library(tidyverse)
library(terra)
library(randomForest)
library(randomForestSRC)
library(patchwork)
library(beepr)
library(here)
library(sf)
library(ggthemes)
here()
options(scipen = 999)
library(randomizr)
source("mytheme.R")
library(tidyverse)
library(terra)
library(randomForest)
library(randomForestSRC)
library(patchwork)
library(beepr)
library(here)
library(sf)
library(ggthemes)
here()
options(scipen = 999)
library(randomizr)
source("mytheme.R")
install.packages("qgisprocess")
library(qgisprocess)
library(terra)
library(sf)
library(qgisprocess)
# Make sure QGIS is set up correctly
qgis_configure()
# ✅ 1. Load and classify the raster
r <- rast("tiled_predictions/pred_map63.tif")
library(here)
library(terra)
library(sf)
library(qgisprocess)
library(here)
# Make sure QGIS is set up correctly
# qgis_configure()
# 1. Load and classify the raster
r <- rast("tiled_predictions/pred_map62.tif")
rmarkdown::find_pandoc()
# Define paths
base_path <- "/Users/RSF/Library/CloudStorage/Dropbox/My Mac (temp’s MacBook Pro)/Documents/GitHub/western-mass-covid"
rmd_path  <- file.path(base_path, "WMA_COVID.Rmd")
docs_path <- file.path(base_path, "docs")
archive_path <- file.path(base_path, "archive")
rmd_path
docs_path
archive_path
# Generate date-stamped filename
today <- format(Sys.Date(), "%Y-%m-%d")
file.path(archive_path, paste0("report-", today, ".html"))
archive_file
archive_file <- file.path(archive_path, paste0("report-", today, ".html"))
archive_file
docs_file <- file.path(docs_path, "index.html")
docs_file
archive_file
# Move to archive with date
file.copy("temp_output.html", archive_file, overwrite = TRUE)
file.path(base_path, "temp_output.html")
# Move to archive with date
file.copy(file.path(base_path, "temp_output.html"), archive_file, overwrite = TRUE)
# Copy to docs/index.html
file.copy(file.path(base_path, "temp_output.html"), docs_file, overwrite = TRUE)
# Clean up
file.remove("temp_output.html")
# Clean up
file.remove(file.path(base_path, "temp_output.html"), "temp_output.html")
file.path(base_path, "temp_output.html")
# Generate date-stamped filename
today <- format(Sys.Date(), "%Y-%m-%d")
archive_file <- file.path(archive_path, paste0("WMA_COVID_", today, ".html"))
docs_file <- file.path(docs_path, "index.html")
# Render to a temporary file
render_output <- rmarkdown::render(
input = rmd_path,
output_file = "temp_output.html"
)
# Move to archive with date
file.copy(file.path(base_path, "temp_output.html"), archive_file, overwrite = TRUE)
# Copy to docs/index.html
file.copy(file.path(base_path, "temp_output.html"), docs_file, overwrite = TRUE)
# Git operations
setwd(base_path)
View(full_sample)
full_sample %>% filter(county %in% wmass_counties) %>% count(county, system)
